{"cells":[{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11404,"status":"ok","timestamp":1684987664822,"user":{"displayName":"Himanshu Pahadia","userId":"12470807699947870598"},"user_tz":240},"id":"4EKbWTD4aQz3","outputId":"150c28ef-ac78-42bd-88b7-10fec324606c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gH-Q4IrYPkQP"},"outputs":[],"source":["%cd drive/MyDrive/PixelTransformer/PixelFormer-main/data_splits"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6952,"status":"ok","timestamp":1684984091071,"user":{"displayName":"Himanshu Pahadia","userId":"12470807699947870598"},"user_tz":240},"id":"Rg10af-QvzuK","outputId":"e876194a-251d-4310-9201-aebaa714cccf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n","Collecting huggingface-hub (from timm)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors (from timm)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n","Installing collected packages: safetensors, huggingface-hub, timm\n","Successfully installed huggingface-hub-0.14.1 safetensors-0.3.1 timm-0.9.2\n"]}],"source":["!pip install timm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLAERTrAvt_P"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","import numpy as np\n","import os\n","import shutil\n","import glob\n","import argparse\n","import numpy as np\n","import random\n","import plotly\n","import plotly.figure_factory as ff\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.utils.data.distributed\n","from torchvision import transforms\n","import time\n","import numpy as np\n","from PIL import Image\n","import os\n","import random\n","import cv2\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9g58AlEyBijK"},"outputs":[],"source":["#Taken inspiration from - Data processing for nyudepth dataset - referenced  https://arxiv.org/pdf/2210.09071.pdf for preprocessing\n","class NewDataLoader(torch.utils.data.Dataset):\n","  def __init__(self, args, mode,transform=None,filename=None):\n","\n","    self.do_kb_crop = False\n","    self.input_height = 480\n","    self.input_width = 640\n","    self.do_random_rotate = True\n","    self.degree = 2.5\n","    self.transform = transform\n","    self.filenames = None\n","\n","    filename = filename\n","    with open(filename, 'r') as f:\n","        self.filenames = f.readlines()\n","        \n","  def __len__(self):\n","        return len(self.filenames)\n","\n","  def __getitem__(self,idx):\n","    if mode =='train':\n","\n","      sample_path = self.filenames[idx]\n","      rgb_file = sample_path.split()[0]\n","      depth_file = sample_path.split()[1]\n","\n","      image_path = rgb_file\n","      depth_path = depth_file\n","      image = Image.open(image_path)\n","      depth_gt = Image.open(depth_path)\n","      \n","\n","      if self.input_height == 480:\n","        depth_gt = np.array(depth_gt)\n","        # print(\"Depth gt\",depth_gt)\n","        valid_mask = np.zeros_like(depth_gt)\n","        valid_mask[45:472, 43:608] = 1\n","        depth_gt[valid_mask==0] = 0\n","        depth_gt = Image.fromarray(depth_gt)\n","      else:\n","        depth_gt = depth_gt.crop((43, 45, 608, 472))\n","        image = image.crop((43, 45, 608, 472))\n","\n","      if self.do_random_rotate is True:\n","                random_angle = (random.random() - 0.5) * 2 * self.degree\n","                image = self.rotate_image(image, random_angle)\n","                depth_gt = self.rotate_image(depth_gt, random_angle, flag=Image.NEAREST)\n","                # depth_gt.show()\n","            \n","      image = np.asarray(image, dtype=np.float32) / 255.0\n","      depth_gt = np.asarray(depth_gt, dtype=np.float32)\n","      depth_gt = np.expand_dims(depth_gt, axis=2)\n","\n","      depth_gt = depth_gt / 1000.0\n","      img, depth = image, depth_gt\n","\n","      #Data processing for nyudepth dataset - referenced  https://arxiv.org/pdf/2210.09071.pdf for preprocessing\n","      H, W = img.shape[0], img.shape[1]\n","      a, b, c, d = random.uniform(0,1), random.uniform(0,1), random.uniform(0,1), random.uniform(0,1)\n","      l, u = int(a*W), int(b*H)\n","      w, h = int(max((W-a*W)*c*0.75, 1)), int(max((H-b*H)*d*0.75, 1))\n","      depth_copied = np.repeat(depth, 3, axis=2)\n","      M = np.ones(img.shape)\n","      M[l:l+h, u:u+w, :] = 0\n","      img = M*img + (1-M)*depth_copied\n","      image = img.astype(np.float32)\n","\n","\n","      if image.shape[0] != self.input_height or image.shape[1] != self.input_width:\n","          image, depth_gt = self.random_crop(image, depth_gt, self.input_height, self.input_width)\n","      image, depth_gt = self.train_preprocess(image, depth_gt)\n","      sample = {'image': image, 'depth': depth_gt}\n","\n","      if self.transform:\n","          sample = self.transform(sample)\n","      \n","      return sample\n","\n","  def rotate_image(self, image, angle, flag=Image.BILINEAR):\n","        result = image.rotate(angle, resample=flag)\n","        return result\n","\n","  def train_preprocess(self, image, depth_gt):\n","        # Random flipping\n","        do_flip = random.random()\n","        if do_flip > 0.5:\n","            image = (image[:, ::-1, :]).copy()\n","            depth_gt = (depth_gt[:, ::-1, :]).copy()\n","    \n","        # Random gamma, brightness, color augmentation\n","        do_augment = random.random()\n","        if do_augment > 0.5:\n","            image = self.augment_image(image)\n","    \n","        return image, depth_gt\n","  def augment_image(self, image):\n","        # gamma augmentation\n","        gamma = random.uniform(0.9, 1.1)\n","        image_aug = image ** gamma\n","\n","        brightness = random.uniform(0.75, 1.25)\n","        image_aug = image_aug * brightness\n","\n","        colors = np.random.uniform(0.9, 1.1, size=3)\n","        white = np.ones((image.shape[0], image.shape[1]))\n","        color_image = np.stack([white * colors[i] for i in range(3)], axis=2)\n","        image_aug *= color_image\n","        image_aug = np.clip(image_aug, 0, 1)\n","\n","        return image_aug\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oNpKnJIZOyuv"},"outputs":[],"source":["class ToTensor(object):\n","    def __init__(self, mode):\n","        self.mode = mode\n","        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","    def to_tensor(self, pic):\n","        if isinstance(pic, np.ndarray):\n","            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n","            return img\n","\n","    def __call__(self, sample):\n","        image = sample['image']\n","        image = self.to_tensor(image)\n","        image = self.normalize(image)\n","        depth = sample['depth']\n","        if self.mode == 'train':\n","            depth = self.to_tensor(depth)\n","            return {'image': image, 'depth': depth}\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKZLr7grOAlJ"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","def preprocessing_transforms(mode):\n","    return transforms.Compose([\n","        ToTensor(mode=mode)\n","    ])\n","mode = 'train'\n","batch_size = 512\n","train_sampler = None\n","num_threads = 1\n","train_file = '/content/drive/MyDrive/CS674-Saha/NewCode/train_values.txt'\n","test_file = '/content/drive/MyDrive/CS674-Saha/NewCode/test_values.txt'\n","training_samples = NewDataLoader([], mode, transform=preprocessing_transforms(mode),filename=train_file)\n","testing_samples = NewDataLoader([], mode, transform=preprocessing_transforms(mode),filename=test_file)\n","train_data = DataLoader(training_samples, batch_size,\n","                                   shuffle=(train_sampler is None),\n","                                   num_workers=1,\n","                                   pin_memory=True,\n","                                   sampler=train_sampler)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CsTEm1IVKP7"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDw5jDwQJvzr"},"outputs":[],"source":["class silog_loss(nn.Module):\n","    def __init__(self, variance_focus):\n","        super(silog_loss, self).__init__()\n","        self.variance_focus = variance_focus\n","\n","    def forward(self, depth_est, depth_gt,mask):\n","        d = torch.log(depth_est[mask]) - torch.log(depth_gt[mask])\n","        return torch.sqrt((d ** 2).mean() - self.variance_focus * (d.mean() ** 2)) * 10.0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"maBbCkq88Yxe"},"outputs":[],"source":["class HuberLoss(nn.Module):\n","    def __init__(self, delta=1.0):\n","        super(HuberLoss, self).__init__()\n","        self.delta = delta\n","\n","    def forward(self, y_pred, y_true):\n","        residual = torch.abs(y_true - y_pred)\n","        condition = residual < self.delta\n","        loss = torch.where(condition, 0.5 * residual ** 2, self.delta * residual - 0.5 * self.delta ** 2)\n","        return torch.mean(loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrv0R6QE_Y0Y"},"outputs":[],"source":["def berhu_loss(pred, target, threshold=0.2):\n","    diff = torch.abs(target - pred)\n","    delta = threshold * torch.max(target).item()\n","    mask = (diff < delta).float()\n","    loss = mask * (diff ** 2 / delta) + (1 - mask) * (diff - 0.5 * delta)\n","    return torch.mean(loss)"]},{"cell_type":"markdown","metadata":{"id":"l9czaWHbXtXU"},"source":["Swin Transformer model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"izFuVlb8ZJmK"},"outputs":[],"source":["def window_reverse(windows, window_size, H, W):\n","    #opposite conversion of windows back to the orginial picture\n","    n = windows.shape[0]\n","    to_restore_h_w = H * W / window_size / window_size\n","    N = int( n / to_restore_h_w) #as n=B*num_windows\n","    H_ =  H // window_size\n","    W_ = W // window_size\n","    x = windows.view(N, H_ , W_, window_size, window_size, -1)\n","    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n","    x = x.view(N, H, W, -1)\n","    return x\n","    \n","def window_partition(x, window_size):\n","    #convert image to windows and returns number of windows with window_shape and channels\n","    N, H, W, C = x.shape\n","    H =  H // window_size\n","    W = W // window_size\n","    x = x.view(N, H, window_size, W , window_size, C) #new height and width based on windows\n","    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C) \n","    #send windows of shape (num_windows*B, window_size, window_size, C)\n","    return windows\n","    \n","\n","class SwinTransformerBlock(nn.Module):\n","    def __init__(self, in_channels, window_size=8, shift_size=8):\n","        super(SwinTransformerBlock,self).__init__()\n","        self.in_channels = in_channels\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.num_heads = 8\n","        self.patch_norm = True\n","        self.p_size = 4\n","        self.patch_size = to_2tuple(self.p_size)\n","        self.embed = 96\n","        self.dim = 96\n","        self.height = 480\n","        self.width = 640\n","\n","\n","        \n","        \n","        self.patch_embed = nn.Conv2d(in_channels,out_channels=self.embed, kernel_size=self.p_size, stride=self.p_size)\n","        \n","        #normalization\n","        self.norm1 = nn.LayerNorm(self.embed)\n","        self.norm2 = nn.LayerNorm(self.embed * 2**0)\n","\n","        #Attention\n","        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=True)\n","        self.attn_drop = nn.Dropout(0)\n","\n","        self.linear = nn.Linear(self.dim, 96)\n","        self.l_drop = nn.Dropout(0)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","        self.fc1 = nn.Linear(self.dim, 4*self.dim)\n","        self.act = nn.GELU()\n","        self.fc2 = nn.Linear(4*self.dim, self.dim)\n","        self.drop = nn.Dropout(0)\n","\n","        # self.change_channels = nn.Conv2d(60, 1, kernel_size=1)\n","\n","        # Define an upsampling operation that increases the spatial dimensions of the tensor by a factor of 3\n","        self.upsample = nn.Upsample(size=(self.height, self.width), mode='bilinear', align_corners=True)\n","\n","\n","        \n","    def forward(self, x):\n","\n","\n","        #perform padding for the images\n","        _, _, H, W = x.size()\n","        if W % self.patch_size[1] != 0:\n","            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n","        if H % self.patch_size[0] != 0:\n","            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n","\n","        #convolution     \n","        x = self.patch_embed(x)\n","        # print(\"X-patch embed\",x.shape)\n","\n","        #layer norm\n","        x = x.permute(0,2,3,1)\n","        x = self.norm1(x)\n","        x = x.permute(0,3,1,2)\n","        # print(\"X-after norm1\",x.shape)\n","\n","\n","        n, c , H , W = x.shape\n","\n","        #Swin Transformer block\n","        residual = x\n","        x = x.permute(0,2,3,1)\n","        x = self.norm2(x)\n","        _,H,W,C = x.shape\n","\n","        # pad images to multiples of window size\n","        pad_l = pad_t = 0\n","        pad_r = pad_b = 0\n","        pad_r = (self.window_size - W % self.window_size) % self.window_size\n","        pad_b = (self.window_size - H % self.window_size) % self.window_size\n","        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","        _, Hp, Wp, _ = x.shape\n","        # print(\"After padding x \",x.shape)\n","\n","        new_x = x\n","        x_windows = window_partition(new_x, self.window_size)\n","        #change the view to calculate attention\n","        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n","        # print(\"After window partition\",x_windows.shape)\n","\n","\n","        #Attention mechanism\n","        B_, N, C = x_windows.shape\n","        qkv = self.qkv(x)\n","\n","        #multi-head attention \n","        qkv = qkv.reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2] \n","        key =  k.transpose(-2, -1)\n","        #tensor multiplication between q and k\n","        attention = (q @ key)\n","        attention = self.softmax(attention)\n","        attention = self.attn_drop(attention)\n","\n","        #tensor multiplication between attn and k\n","        x = (attention @ v).transpose(1, 2).reshape(B_, N, C)\n","        x = self.linear(x)\n","        x = self.l_drop(x)\n","        # print(\"After attention\", x.shape)\n","\n","        #Merge windows back to picture\n","        attn_windows = x.view(-1, self.window_size, self.window_size, C)\n","        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)\n","\n","        x = shifted_x\n","        residual = residual.permute(0,2,3,1)\n","        #perform residual padding\n","        residual = F.pad(residual, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","        residual = residual.permute(0,3,1,2)\n","        x = x.permute(0,3,1,2)\n","\n","        #add residual to x\n","        x = residual + x\n","\n","        #cache current x\n","        residual = x\n","        x = x.permute(0,2,3,1)\n","        x = self.norm2(x)\n","\n","        #perform MLP\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        x = x.permute(0,3,1,2)\n","        \n","        #add residual again\n","        x = x + residual\n","        N, C, H, W = x.shape\n","        x = x.permute(0,2,3,1)\n","\n","        #Downsample and reverse padding\n","        pad_input = (H % 2 == 1) or (W % 2 == 1)\n","        if pad_input:\n","            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n","\n","        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n","        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n","        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n","        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n","        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n","        _,H1,W1,_ = x.shape\n","\n","        #upsample to get back the image shape \n","        x = self.upsample(x)\n","        # print(\"After upsample\",x.shape)\n","\n","        #change channels back to depth shape\n","        # x = self.change_channels(x)\n","        # print(\"After upsample\",x.shape)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"FcpDH9jJXo-q"},"source":["PSP ( Pyramid Spatial Pooling ) model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzWN5_W8-vtw"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class PSPModule(nn.Module):\n","    def __init__(self, in_channels, sizes=(1, 2, 3, 6)):\n","        super(PSPModule, self).__init__()\n","\n","        #used to perform 4 different types of adaptive conv by scaling using 1,2,3,6 sizes\n","        self.stages = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv2d(in_channels, in_channels // len(sizes), kernel_size=1),\n","                nn.BatchNorm2d(in_channels // len(sizes)),\n","                nn.ReLU()\n","            )\n","            for size in sizes\n","        ])\n","        self.upsample = nn.Conv2d(in_channels=120,out_channels=60,kernel_size = 1)\n","\n","    def forward(self, x):\n","        h, w = x.shape[2],x.shape[3]\n","        features = [x]\n","        for stage in self.stages:\n","            out_stage = F.interpolate( #used to upsample and concatenate\n","                stage(x),\n","                size=(h, w), \n","                mode='bilinear', #uses neighbour four pixels\n","                align_corners=True\n","            )\n","            features.append(out_stage)\n","        #concatenates the features together\n","        concat = torch.cat(features, dim=1)\n","        return(self.upsample(concat))\n"]},{"cell_type":"markdown","metadata":{"id":"gd3j2djLXmyl"},"source":["Decoder SAM model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lzWnjAyOcyi"},"outputs":[],"source":["class SAM(nn.Module):\n","    def __init__(self, in_channels,out_channels, window_size=8, shift_size=8):\n","        super(SAM,self).__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.num_heads = 8\n","        self.patch_norm = True\n","        self.p_size = 4\n","        self.patch_size = to_2tuple(self.p_size)\n","        self.embed = 96\n","        self.dim = self.embed * 2**0\n","        self.height = 480\n","        self.width = 640\n","\n","\n","        \n","        \n","        self.patch_embed = nn.Conv2d(in_channels,out_channels=self.embed, kernel_size=self.p_size, stride=self.p_size)\n","        \n","        self.norm1 = nn.LayerNorm(self.embed)\n","        self.norm2 = nn.LayerNorm(self.embed * 2**0)\n","        #Attention\n","        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=True)\n","        self.attn_drop = nn.Dropout(0)\n","        self.proj = nn.Linear(self.dim, self.dim)\n","        self.proj_drop = nn.Dropout(0)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","        self.fc1 = nn.Linear(self.dim, 4*self.dim)\n","        self.act = nn.GELU()\n","        self.fc2 = nn.Linear(4*self.dim, self.dim)\n","        self.drop = nn.Dropout(0)\n","\n","      \n","        self.change_channels = nn.Conv2d(60, 1, kernel_size=1)\n","\n","        # Define an upsampling operation that increases the spatial dimensions of the tensor by a factor of 3\n","        self.upsample = nn.Upsample(size=(self.height, self.width), mode='bilinear', align_corners=True)\n","\n","\n","        \n","    def forward(self, x,y):\n","\n","\n","        #same as Swin Transformer but twice once for out_swin and other for out_psp\n","        #-----------------------For E----------------------\n","        _, _, H, W = x.size()\n","        if W % self.patch_size[1] != 0:\n","            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n","        if H % self.patch_size[0] != 0:\n","            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n","        x = self.patch_embed(x)\n","\n","\n","        x = x.permute(0,2,3,1)\n","        x = self.norm1(x)\n","        x = x.permute(0,3,1,2)\n","        # print(\"X-after norm1\",x.shape)\n","\n","\n","        n, c , H , W = x.shape\n","        # residual = x\n","        x = x.permute(0,2,3,1)\n","        x = self.norm2(x)\n","        _,H,W,C = x.shape\n","        pad_l = pad_t = 0\n","        pad_r = pad_b = 0\n","        pad_r = (self.window_size - W % self.window_size) % self.window_size\n","        pad_b = (self.window_size - H % self.window_size) % self.window_size\n","\n","        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","        _, Hp, Wp, _ = x.shape\n","        # print(\"After padding x \",x.shape)\n","\n","        shifted_x = x\n","        attn_mask = None\n","        x_windows = window_partition(shifted_x, self.window_size)\n","        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n","        # print(\"After window partition\",x_windows.shape)\n","\n","\n","        #-------------------For Q-----------------------------\n","        _, _, H, W = y.size()\n","        if W % self.patch_size[1] != 0:\n","            y = F.pad(y, (0, self.patch_size[1] - W % self.patch_size[1]))\n","        if H % self.patch_size[0] != 0:\n","            y = F.pad(y, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n","        y = self.patch_embed(y)\n","        # print(\"X-patch embed\",x.shape)\n","\n","\n","        y = y.permute(0,2,3,1)\n","        y = self.norm1(y)\n","        y = y.permute(0,3,1,2)\n","        # print(\"X-after norm1\",x.shape)\n","\n","\n","        n, c , H , W = y.shape\n","\n","\n","        #Swin Transformer block\n","        residual = y\n","        y = y.permute(0,2,3,1)\n","        y = self.norm2(y)\n","        _,H,W,C = y.shape\n","        pad_l = pad_t = 0\n","        pad_r = pad_b = 0\n","        pad_r = (self.window_size - W % self.window_size) % self.window_size\n","        pad_b = (self.window_size - H % self.window_size) % self.window_size\n","        # print(self.window_size)\n","        # print(pad_r)\n","        y = F.pad(y, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","        _, Hp, Wp, _ = y.shape\n","        # print(\"After padding x \",x.shape)\n","\n","        shifted_y = y\n","        attn_mask = None\n","        y_windows = window_partition(shifted_y, self.window_size)\n","        y_windows = y_windows.view(-1, self.window_size * self.window_size, C)\n","\n","        #-----------CROSS ATTENTION------\n","\n","        #Attention\n","        B_,N,C = x_windows.shape #E\n","        B_1, N1, C1 = y_windows.shape #Q\n","\n","        qkv_E = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        qkv_Q = self.qkv(y).reshape(B_1, N1, 3, self.num_heads, C1// self.num_heads).permute(2, 0, 3, 1, 4)\n","        q = qkv_Q[0]\n","        k ,v = qkv_E[1], qkv_E[2]\n","        # print(\"Query\",q.shape)\n","        # print(\"Key\",k.shape)\n","        # print(\"value\",v.shape)\n","        attn = (q @ k.transpose(-2, -1))\n","        attn = self.softmax(attn)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        # print(\"After attention\", x.shape)\n","\n","        #Merge windows\n","        attn_windows = x.view(-1, self.window_size, self.window_size, C)\n","        # print(\"After merging\",attn_windows.shape)\n","        new_x = window_reverse(attn_windows, self.window_size, Hp, Wp)\n","        # print(\"After reverse shift\",shifted_x.shape)\n","        # x = shifted_x\n","\n","        # x = x.view(n, H * W, C)\n","        residual = residual.permute(0,2,3,1)\n","        residual = F.pad(residual, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","        x = new_x\n","        residual = residual.permute(0,3,1,2)\n","        x = x.permute(0,3,1,2)\n","\n","        #Add residual\n","        x = residual + x\n","\n","        residual = x\n","        x = x.permute(0,2,3,1)\n","        \n","        #Perform MLP\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        x = x.permute(0,3,1,2)\n","        \n","        #Add new residual\n","        x = x + residual\n","\n","        N, C, H, W = x.shape\n","        x = x.permute(0,2,3,1)\n","        #Downsample and reverse padding\n","        pad_input = (H % 2 == 1) or (W % 2 == 1)\n","        if pad_input:\n","            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n","\n","        x0 = x[:, 0::2, 0::2, :] \n","        x1 = x[:, 1::2, 0::2, :] \n","        x2 = x[:, 0::2, 1::2, :] \n","        x3 = x[:, 1::2, 1::2, :]  \n","        x = torch.cat([x0, x1, x2, x3], -1)  \n","        _,H1,W1,_ = x.shape\n","      \n","        #used to change number of channels\n","        x = self.change_channels(x)\n","        # print(\"After channel switch\",x.shape)\n","\n","        #used to upsample the width and height to original dimensions\n","        x = self.upsample(x)\n","        # print(\"After upsample\",x.shape)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDFrD2kqkAGt"},"outputs":[],"source":["def compute_errors(gt, pred):\n","\n","    abs_diff = np.abs(gt - pred)\n","    # d1_error = abs_diff > (1.25)\n","\n","    d1 = 0\n","\n","    rms = (gt - pred) ** 2\n","    rms = np.sqrt(torch.mean(rms))\n","    log_rms = (np.log(gt) - np.log(pred)) ** 2\n","    log_rms = np.sqrt(torch.mean(log_rms))\n","\n","    abs_rel = torch.mean(np.abs(gt - pred) / gt)\n","    sq_rel = torch.mean(((gt - pred) ** 2) / gt)\n","    \n","    err = np.log(pred) - np.log(gt)\n","    # silog = np.sqrt(torch.mean(err ** 2) - torch.mean(err) ** 2) * 100\n","    # print(\"RMS\",err)\n","    err = np.abs(np.log10(pred) - np.log10(gt))\n","    log10 = torch.mean(err)\n","    # print(\"RMS\",log10)\n","\n","    return [ abs_rel, log10, rms, sq_rel, log_rms, d1]"]},{"cell_type":"markdown","metadata":{"id":"NgKvF0nLXj_e"},"source":["ResNet block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7qkjghcV0p6"},"outputs":[],"source":["import torch.nn as nn\n","#used layer norm instead of batch norm for batch_size=1\n","class ResNetBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super(ResNetBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.LayerNorm(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.LayerNorm(out_channels)\n","        self.residual_change_conv  = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n","        self.residual_change_ln =  nn.LayerNorm(out_channels)\n","\n","        self.change_channels = nn.Conv2d(64, 3, kernel_size=1)\n","\n","    def forward(self, x):\n","        residual = x\n","        x = self.conv1(x)\n","\n","        x = x.permute(0,2,3,1)\n","\n","        x = self.bn1(x)\n","        x = x.permute(0,3,1,2)\n","        x = self.relu(x)\n","        x = self.conv2(x)\n","        x = x.permute(0,2,3,1)\n","\n","        x = self.bn2(x)\n","        x = x.permute(0,3,1,2)\n","        residual = self.residual_change_conv(residual)\n","        residual = residual.permute(0,2,3,1)\n","\n","        residual = self.residual_change_ln(residual)\n","        residual = residual.permute(0,3,1,2)\n","        x += residual\n","        x = self.relu(x)\n","\n","        #change to give input to the swin transformer\n","        out = self.change_channels(x)\n","        return out\n"]},{"cell_type":"markdown","metadata":{},"source":["Training and Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A09Z0B3HPOeh"},"outputs":[],"source":["# Training parameters\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","variance_focus = 0.1\n","learning_rate = 0.01\n","num_epochs = 5\n","batch_size = 20\n","\n","torch.cuda.manual_seed(42)\n","\n","model = SwinTransformerBlock(in_channels=3,window_size=8).to(device)\n","model_resnet = ResNetBlock(in_channels=3,out_channels=64).to(device)\n","model_PSPNet = PSPModule(in_channels=60).to(device) #divisible by all sizes 1,2,3,6\n","model_SAM = SAM(in_channels=60, out_channels= 1,window_size=8).to(device)\n","\n","\n","change_channels = nn.Conv2d(60, 1, kernel_size=1)\n","params_ = list(model.parameters())+ list(model_resnet.parameters())+list(model_PSPNet.parameters()) +list(model_SAM.parameters())\n","optimizer = torch.optim.Adam([{'params': params_}],lr=learning_rate)\n","silog_criterion = silog_loss(variance_focus=variance_focus)\n","huber_loss = HuberLoss(delta=0.5)\n","train_images = []\n","train_target = []\n","\n","for epoch in range(num_epochs):\n","  # checkpoint_path1 = '/content/drive/MyDrive/PixelTransformer/PixelFormer-main/data_splits/resnet2/checkpoint'+str(epoch)+'.pt'\n","  # checkpoint_path2 = '/content/drive/MyDrive/PixelTransformer/PixelFormer-main/data_splits/swin2/checkpoint'+str(epoch)+'.pt'\n","  # checkpoint_path3 = '/content/drive/MyDrive/PixelTransformer/PixelFormer-main/data_splits/psp2/checkpoint'+str(epoch)+'.pt'\n","  checkpoint_path1 = '/content/drive/MyDrive/CS674-Saha/NewCode/models/resnet2/checkpoint'+str(epoch)+'.pt'\n","  checkpoint_path2 = '/content/drive/MyDrive/CS674-Saha/NewCode/models/swin2/checkpoint'+str(epoch)+'.pt'\n","  checkpoint_path3 = '/content/drive/MyDrive/CS674-Saha/NewCode/models/psp2/checkpoint'+str(epoch)+'.pt'\n","  checkpoint_path4 = '/content/drive/MyDrive/CS674-Saha/NewCode/models/sam2/checkpoint'+str(epoch)+'.pt'\n","  train_loss = 0\n","  for val in training_samples:\n","      image = torch.autograd.Variable(val['image']).to(device) #gpu\n","      depth_gt = torch.autograd.Variable(val['depth']).to(device)\n","      # model.train()\n","      optimizer.zero_grad()\n","      \n","      image = image.unsqueeze(0).to(device)\n","      depth_gt = depth_gt.unsqueeze(0).to(device)\n","\n","      #image goes through resnet\n","      resnet_image = model_resnet.train()(image)\n","      \n","      #output goes through swin transformer\n","      out_swin = model.train()(resnet_image) #Swin\n","      # print(\"Ater swin\",out_swin.shape) #60\n","\n","      #output from swin goes through PSP \n","      out_psp = model_PSPNet.train()(out_swin)\n","      # depth_est = change_channels(out_psp.cpu()).to(device)\n","      # print(\"After psp\",out_psp.shape) #120\n","\n","      #output from PSP and swin goes through Decoder SAM \n","      depth_est = model_SAM.train()(out_swin,out_psp)\n","      # print(\"After SAM\",depth_est.shape) #120\n","\n","      #used to prevent log 0 error\n","      mask =  (depth_est>0.1) & (depth_gt>0.1)\n","      # l1 = silog_criterion.forward(depth_est,depth_gt,mask.to(torch.bool))\n","      l1 = huber_loss(depth_est[mask],depth_gt[mask])\n","      # l1 = berhu_loss(depth_est,depth_gt)\n","\n","      train_loss+=l1\n","      l1.backward()\n","      optimizer.step()\n","      # print(\"Train loss\",l1)\n","  # model.eval()\n","  val_loss = 0.0\n","  val_acc = 0.0\n","  # Iterate over the validation data loader\n","\n","  rae ,log10 , rms, sql_rel, log_rms, d1 = 0,0,0,0,0,0\n","\n","  with torch.no_grad():\n","      for val in testing_samples:\n","          image = torch.autograd.Variable(val['image'])\n","          depth_gt = torch.autograd.Variable(val['depth']) #gpu\n","          # Forward pass\n","          image = image.unsqueeze(0).to(device)\n","          depth_gt = depth_gt.unsqueeze(0).to(device)\n","\n","          #image goes through resnet\n","          resnet_image = model_resnet.eval()(image)\n","\n","          #output goes through swin transformer\n","          out_swin = model.eval()(resnet_image) #Swin\n","          # print(\"Ater swin\",out_swin.shape) #60\n","\n","          #output from swin goes through PSP \n","          out_psp = model_PSPNet.eval()(out_swin)\n","          # depth_est = change_channels(out_psp.cpu()).to(device)\n","          # print(\"After psp\",out_psp.shape) #120\n","\n","          #output from PSP and swin goes through Decoder SAM \n","          depth_est = model_SAM.eval()(out_swin,out_psp)\n","\n","          #used to prevent log 0 error\n","          mask =  (depth_est>0.1) & (depth_gt>0.1)\n","          # loss = silog_criterion.forward(depth_est, depth_gt,mask.to(torch.bool))\n","          loss = huber_loss(depth_est[mask],depth_gt[mask])\n","          # loss = berhu_loss(depth_est,depth_gt)\n","\n","\n","          val_loss += loss.item() \n","          # Compute the accuracy\n","          erros_measures = compute_errors(depth_est[mask.to(torch.bool)].cpu(),depth_gt[mask.to(torch.bool)].cpu())\n","          rae += erros_measures[0]\n","          log10 += erros_measures[1]\n","          rms += erros_measures[2]\n","          # sql_rel += erros_measures[3]\n","          log_rms += erros_measures[4]\n","          # d1 += erros_measures[5]\n","      print(\"Average error measures for val dataset:\")\n","      print(\"Absolute error {:.4f} Log10 {:.4f} RMS {:.4f}  log_rms {:.4f} \".format(rae/len(testing_samples),log10/len(testing_samples),rms/len(testing_samples),log_rms/len(testing_samples)))\n","  torch.save(model_resnet.state_dict(), checkpoint_path1)\n","  torch.save(model.state_dict(), checkpoint_path2)\n","  torch.save(model_PSPNet.state_dict(), checkpoint_path3)\n","  torch.save(model_SAM.state_dict(), checkpoint_path4)\n","  # Compute the average validation loss and accuracy\n","  val_loss /= len(testing_samples)\n","  train_loss /=len(training_samples)\n","\n","  # Print the epoch number, training loss, and validation loss and accuracy\n","  print(\"Epoch {}: train_loss {:.4f} val_loss {:.4f}\".format(epoch+1, train_loss, val_loss))\n","    "]},{"cell_type":"markdown","metadata":{"id":"cofAPjSYTBmU"},"source":["For Testing purpose"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72302,"status":"ok","timestamp":1684988424440,"user":{"displayName":"Himanshu Pahadia","userId":"12470807699947870598"},"user_tz":240},"id":"lhmyNQaHTGUG","outputId":"57f252ce-e4de-486a-8630-8db9360aedac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average error measures for val dataset:\n","Absolute error 0.3797 Log10 0.1538 RMS 1.1649  log_rms 0.4244 \n"," val_loss 0.3552\n"]}],"source":["# Training parameters\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","torch.cuda.manual_seed(42)\n","variance_focus = 0.1\n","learning_rate = 0.01\n","num_epochs = 5\n","batch_size = 20\n","\n","model = SwinTransformerBlock(in_channels=3,window_size=8).to(device)\n","model_resnet = ResNetBlock(in_channels=3,out_channels=64).to(device)\n","model_PSPNet = PSPModule(in_channels=60).to(device) #divisible by all sizes 1,2,3,6\n","model_SAM = SAM(in_channels=60, out_channels= 1,window_size=8).to(device)\n","\n","\n","change_channels = nn.Conv2d(60, 1, kernel_size=1)\n","\n","# optimizer = torch.optim.Adam([{'params': model.parameters()}],lr=learning_rate)\n","silog_criterion = silog_loss(variance_focus=variance_focus)\n","huber_loss = HuberLoss(delta=0.5)\n","\n","#for best checkpoint\n","\n","# for epoch in range(num_epochs):\n","train_loss = 0\n","model_resnet.load_state_dict(torch.load('checkpoint-resnet.pt'))\n","model.load_state_dict(torch.load('checkpoint-swin.pt'))\n","model_PSPNet.load_state_dict(torch.load('checkpoint-psp.pt'))\n","model_SAM.load_state_dict(torch.load('checkpoint-sam.pt'))\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","for param in model_resnet.parameters():\n","    param.requires_grad = False\n","\n","for param in model_PSPNet.parameters():\n","    param.requires_grad = False\n","\n","\n","for param in model_SAM.parameters():\n","    param.requires_grad = False\n","\n","\n","val_loss = 0.0\n","val_acc = 0.0\n","# Iterate over the validation data loader\n","\n","rae ,log10 , rms, sql_rel, log_rms, d1 = 0,0,0,0,0,0\n","\n","with torch.no_grad():\n","    for val in testing_samples:\n","        image = torch.autograd.Variable(val['image'])\n","        depth_gt = torch.autograd.Variable(val['depth']) #gpu\n","        # Forward pass\n","        image = image.unsqueeze(0).to(device)\n","        depth_gt = depth_gt.unsqueeze(0).to(device)\n","\n","        #image goes through resnet\n","        resnet_image = model_resnet.eval()(image)\n","\n","        #output goes through swin transformer\n","        out_swin = model.eval()(resnet_image) #Swin\n","        # print(\"Ater swin\",out_swin.shape) #60\n","\n","        #output from swin goes through PSP \n","        out_psp = model_PSPNet.eval()(out_swin)\n","        # depth_est = change_channels(out_psp.cpu()).to(device)\n","        # print(\"After psp\",out_psp.shape) #120\n","\n","        #output from PSP and swin goes through Decoder SAM \n","        depth_est = model_SAM.eval()(out_swin,out_psp)\n","\n","        #used to prevent log 0 error\n","        mask =  (depth_est>0.1) & (depth_gt>0.1)\n","        # loss = silog_criterion.forward(depth_est, depth_gt,mask.to(torch.bool))\n","        loss = huber_loss(depth_est[mask],depth_gt[mask])\n","        # loss = berhu_loss(depth_est,depth_gt)\n","\n","\n","        val_loss += loss.item() \n","        #print(val_loss)\n","        # Compute the accuracy\n","        erros_measures = compute_errors(depth_est[mask.to(torch.bool)].cpu(),depth_gt[mask.to(torch.bool)].cpu())\n","        rae += erros_measures[0]\n","        log10 += erros_measures[1]\n","        rms += erros_measures[2]\n","        # sql_rel += erros_measures[3]\n","        log_rms += erros_measures[4]\n","        # d1 += erros_measures[5]\n","    print(\"Average error measures for val dataset:\")\n","    print(\"Absolute error {:.4f} Log10 {:.4f} RMS {:.4f}  log_rms {:.4f} \".format(rae/len(testing_samples),log10/len(testing_samples),rms/len(testing_samples),log_rms/len(testing_samples)))\n","# Compute the average validation loss and accuracy\n","val_loss /= len(testing_samples)\n","# train_loss /=len(training_samples)\n","\n","# Print the epoch number, training loss, and validation loss and accuracy\n","print(\" val_loss {:.4f}\".format( val_loss))\n","  "]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
